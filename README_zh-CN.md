<div align="center">

# HammerLLMğŸ”¨

ä¸­æ–‡ | [English](README.md)

</div>

<h5 align=center>

[![hf](https://img.shields.io/badge/ğŸ¤—-Hugging%20Face-blue.svg)](https://huggingface.co/collections/DataHammer/hammerllm-14b-660d227bf2e7fcbf6ceb5620)
[![License](https://img.shields.io/badge/Code%20License-MIT-yellow)](https://github.com/Academic-Hammer/HammerLLM/blob/main/LICENSE)
[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FAcademic-Hammer%2FHammerLLM&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=Visitor&edge_flat=false)](https://hits.seeyoufarm.com)
[![GitHub stars](https://img.shields.io/github/stars/Academic-Hammer/HammerLLM.svg?colorA=orange&colorB=orange&logo=github)](https://github.com/Academic-Hammer/HammerLLM/stargazers)
</h5>

æ¬¢è¿æ¥åˆ°æˆ‘ä»¬çš„å…·æœ‰1.4Bå‚æ•°é‡çš„LLM â€”â€” HammerLLMçš„ä»£ç å’Œèµ„æºåº“ã€‚å’Œå·²æœ‰çš„small-size LLMå¯¹æ¯”ä¸‹ï¼Œæˆ‘ä»¬çš„HammerLLMåœ¨ä»¥ä¸‹ç‰¹æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼š
1. å…¼å®¹Llama-2çš„ä¸­è‹±æ–‡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ğŸ¦™
2. ç®€æ´é«˜æ•ˆçš„è®­ç»ƒä»£ç åº“ğŸš€
3. å®Œå…¨å¼€æºï¼šæ¨¡å‹æƒé‡ã€ç¯å¢ƒã€ä»£ç åº“å’Œè¶…å‚æ•°ğŸ”—
4. ä¸å…·æœ‰ç›¸ä¼¼æ¨¡å‹è§„æ¨¡çš„å…ˆè¿›sLLMç›¸æ¯”å…·æœ‰å¯æ¯”è¾ƒçš„æ€§èƒ½ğŸ¥‡
5. å…·æœ‰æœ€é«˜å‹ç¼©ç‡å’Œ100%ä¸­æ–‡æ±‰å­—è¦†ç›–ç‡çš„åˆ†è¯å™¨ğŸ†


## ğŸ”¥What's New!

* **[2024.4.2]** ğŸ‰ğŸ‰ğŸ‰å‘å¸ƒäº†å…³äºæˆ‘ä»¬çš„HammerLLMğŸ”¨çš„æ‰€æœ‰èµ„æºï¼ŒåŒ…æ‹¬æ¨¡å‹æƒé‡ã€Dockerç¯å¢ƒä»¥åŠè®­ç»ƒä»£ç åº“ã€‚


## ğŸ§¾ Next

- [ ] ç»§ç»­åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸Šè®­ç»ƒæ›´å¤šçš„ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å­—ç¬¦å’Œä»£ç æ•°æ®

## Table of Contents

- [HammerLLMğŸ”¨](#hammerllm)
  - [ğŸ”¥What's New!](#whats-new)
  - [ğŸ§¾ Next](#-next)
  - [Table of Contents](#table-of-contents)
  - [âš™ï¸Inference](#ï¸inference)
    - [Example Code](#example-code)
  - [ğŸ™ŒCases](#cases)
    - [Chinese Generation](#chinese-generation)
    - [English Generation](#english-generation)
    - [Code Generation](#code-generation)
  - [ğŸ¤—Pretrained Models](#pretrained-models)
  - [ğŸ“ŠPerformance](#performance)
    - [Perplexity on Latest Test Dataset](#perplexity-on-latest-test-dataset)
      - [Chinese Corpus (Skywork-ChineseDomainModelingEval)](#chinese-corpus-skywork-chinesedomainmodelingeval)
      - [English Corpus (RealTimeData-Latest Collection)](#english-corpus-realtimedata-latest-collection)
    - [ğŸ““Tokenizer](#tokenizer)
    - [â©Acceleration](#acceleration)
  - [ğŸ“ˆTraining Progress](#training-progress)
  - [â™»ï¸Reproduce Our Work](#ï¸reproduce-our-work)
  - [ğŸ–Šï¸Citing](#ï¸citing)
  - [ğŸ¤Contact Us](#contact-us)
  - [ğŸ’³License](#license)
  - [ğŸ’ªContributing](#contributing)

## âš™ï¸Inference

### Example Code
å¦‚ä¸‹ä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨**HuggingFace** `transformers`ä¸æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œäº¤äº’ï¼š


```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = 'DataHammer/hammerllm-1.4b-222k'
text = 'åŒ—äº¬ç†å·¥å¤§å­¦æ˜¯'
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
# if your device donot support the bfloat16, you could remove it
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)

input_ids = tokenizer(text, return_tensors='pt').input_ids
output = model.generate(
    input_ids=input_ids.cuda(),
    max_length=min(int(len(input_ids) + 100), 1024),
    do_sample=True,
    top_p=0.95
).tolist()

generation = tokenizer.decode(output[0])
print(generation)
```

## ğŸ™ŒCases
æˆ‘ä»¬è¿˜å±•ç¤ºäº†æ¨¡å‹åœ¨ä¸­æ–‡ã€è‹±æ–‡å’Œä»£ç è®¾ç½®ä¸Šä½œä¸ºbase modelå®Œæˆæ–‡æœ¬è¡¥å…¨çš„ä¸€äº›caseã€‚


### Chinese Generation

```
Prompt: åŒ—äº¬ç†å·¥å¤§å­¦æ˜¯
Generation: <s> åŒ—äº¬ç†å·¥å¤§å­¦æ˜¯æ•™è‚²éƒ¨ç›´å±å…¨å›½é‡ç‚¹å¤§å­¦ï¼Œæ˜¯æˆ‘å›½â€œ211å·¥ç¨‹â€å’Œâ€œ985å·¥ç¨‹â€é‡ç‚¹å»ºè®¾çš„æ•™è‚²éƒ¨ç›´å±ç†å·¥ç±»å…¨å›½é‡ç‚¹å¤§å­¦ï¼Œç›´å±äºå·¥ä¸š å’Œä¿¡æ¯åŒ–éƒ¨ï¼Œæ˜¯å…¨å›½é¦–æ‰¹æ­£å¼è·å¾—åšå£«å­¦ä½å’Œåšå£«åç ”ç©¶èµ„æ ¼çš„é«˜æ ¡ä¹‹ä¸€ï¼Œé¦–æ‰¹è·å¾—å…¬å®‰ä¸“ä¸šç¡•å£«ã€å…¬å®‰ä¸“ä¸šåšå£«å’Œå¸æ³•ä¸“ä¸šåšå£«å­¦ä½çš„22æ‰€é«˜æ ¡ä¹‹ ä¸€ï¼Œæ˜¯åšå£«å­¦ä½æˆäºˆå•ä½ã€æ³•å­¦ï¼ˆä¸€çº§å­¦ç§‘ï¼‰å’Œå…¬å®‰å­¦ï¼ˆä¸€çº§å­¦ç§‘ï¼‰åšå£«æˆæƒé«˜æ ¡ï¼Œæ˜¯ä¸€æ‰€ä»¥å·¥ä¸ºä¸»ã€ç†å·¥ç»“åˆã€å¤šå­¦ç§‘åè°ƒå‘å±•çš„å…¨å›½é‡ç‚¹å¤§å­¦ã€‚
åŒ—äº¬ç†å·¥å¤§å­¦æ˜¯ä¸­å¤®éƒ¨é—¨æ‰€å±é«˜æ ¡ï¼Œä¸­å¤®å…±å»ºé«˜æ ¡ï¼Œæ•™è‚²éƒ¨ç›´å±é«˜æ ¡ã€‚
ä¸€ã€å¤§å­¦é™¢ï¼ˆé™¢ç³»ï¼‰
åŒ—äº¬ç†å·¥å¤§å­¦ç°æœ‰48ä¸ªåšå£«å­¦ä½æˆæƒä¸€çº§å­¦ç§‘ï¼Œæ¶µç›–17ä¸ªå­¦ç§‘é—¨ç±»ã€‚
äºŒã€ä¸“ä¸šé™¢ç³»
åŒ—äº¬ç†å·¥å¤§å­¦è®¾æœ‰11ä¸ªä¸€çº§å­¦ç§‘åšå£«åç§‘ç ”æµåŠ¨ç«™
ä¸‰ã€ç ”ç©¶ç”Ÿé™¢
åŒ—äº¬ç†å·¥å¤§å­¦è®¾æœ‰29ä¸ªç ”ç©¶æ‰€ï¼Œå…¶ä¸­å›½å®¶é‡ç‚¹å®éªŒå®¤2ä¸ªï¼› åšå£«åç§‘ç ”æµåŠ¨ç«™18ä¸ªï¼›åšå£«åç§‘ç ”å·¥ä½œç«™3ä¸ªã€‚
åŒ—äº¬ç†å·¥å¤§å­¦ç ”ç©¶ç”Ÿé™¢æ‹›ç”Ÿä¸“ä¸šï¼š</s>
```

```
Prompt: å°ç±³å…¬å¸æ˜¯ä¸€å®¶
Generation: <s> å°ç±³å…¬å¸æ˜¯ä¸€å®¶ä¸“æ³¨äºæ™ºèƒ½ç¡¬ä»¶å’Œç”µå­äº§å“ç ”å‘çš„äº’è”ç½‘å…¬å¸ï¼Œå…¬å¸äº§å“åŒ…æ‹¬æ™ºèƒ½æ‰‹æœºã€ç¬”è®°æœ¬ç”µè„‘ã€è·¯ç”±å™¨ç­‰ã€‚å°ç±³å…¬å¸ä¸€ç›´ä¸“ æ³¨äºæŠ€æœ¯åˆ›æ–°å’Œå“è¶Šçš„ä½“éªŒï¼Œè‡´åŠ›äºä¸ºç”¨æˆ·æä¾›æ›´å¥½çš„äº§å“ã€‚
å°ç±³å…¬å¸æ‹¥æœ‰ä¸€æ”¯ç»éªŒä¸°å¯Œçš„ç ”å‘å›¢é˜Ÿï¼Œåœ¨äº§å“å¼€å‘ã€ç”¨æˆ·ä½“éªŒç­‰æ–¹é¢ç§¯ç´¯äº†ä¸°å¯Œçš„ç»éªŒã€‚å…¬å¸ç ”å‘å›¢é˜Ÿä¸­çš„å¤§éƒ¨åˆ†æˆå‘˜æ¥è‡ªå¾®è½¯ã€è°·æ­Œç­‰çŸ¥åç§‘æŠ€ å…¬å¸ï¼Œå¯¹äº’è”ç½‘å’Œç§»åŠ¨äº’è”ç½‘æŠ€æœ¯æœ‰ç€æ·±åˆ»ç†è§£ã€‚æ­¤å¤–ï¼Œå°ç±³å…¬å¸è¿˜æ‹¥æœ‰ä¸“ä¸šçš„å¼€å‘å›¢é˜Ÿå’Œè‰¯å¥½çš„æŠ€æœ¯æ”¯æŒã€‚
å°ç±³å…¬å¸çš„æ ¸å¿ƒäº§å“ä¹‹ä¸€æ˜¯å°ç±³æ‰‹æœºã€‚å°ç±³å…¬å¸çš„ç›®æ ‡æ˜¯è®©ç”¨æˆ·å¯ä»¥åœ¨æ‰‹æœºä¸­è½»æ¾åˆ†äº«å›¾ç‰‡ã€è§†é¢‘ã€éŸ³ä¹å’ŒéŸ³ä¹æ–‡ä»¶ç­‰ã€‚å°ç±³å…¬å¸æä¾›äº†ä¸€ç§ç®€å•æ˜“ ç”¨çš„ç§»åŠ¨åª’ä½“è§£å†³æ–¹æ¡ˆï¼Œè®©ç”¨æˆ·å¯ä»¥åˆ©ç”¨æ™ºèƒ½æ‰‹æœºæ¥è½»æ¾äº«å—éŸ³ä¹ã€ç…§ç‰‡ã€è§†é¢‘å’Œæ¸¸æˆç­‰ç§»åŠ¨åª’ä½“å†…å®¹ã€‚
å°ç±³æ‰‹æœºè¿˜æ”¯æŒå¤šç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€è¿œç¨‹æ§åˆ¶ã€è¿œç¨‹å®šä½ã€ç§»åŠ¨æ”¯ä»˜ç­‰ã€‚å°ç±³æ‰‹æœºè¿˜æ”¯æŒå„ç§ä¸åŒçš„åº”ç”¨å’ŒæœåŠ¡ï¼ŒåŒ…æ‹¬ç¤¾äº¤åª’ä½“ã€æ¸¸æˆã€ è§†é¢‘åˆ†äº«ã€æœ¬åœ°åŒ–æœåŠ¡ç­‰ã€‚
å°ç±³å…¬å¸è¿˜æ‹¥æœ‰ä¸°å¯Œçš„ç¡¬ä»¶äº§å“çº¿ï¼ŒåŒ…æ‹¬æ™ºèƒ½æ‰‹æœºã€ç¬”è®°æœ¬ç”µè„‘ã€å¹³æ¿ç”µè„‘ã€ç”µè§†ç­‰ã€‚å°ç±³å…¬å¸è¿˜æ¨å‡ºäº†è®¸å¤šæå…·ç«äº‰åŠ›çš„äº§å“ï¼Œå¦‚æ™ºèƒ½æ‰‹ç¯ã€æ™ºèƒ½ æ‰‹è¡¨ç­‰ã€‚</s>
```

### English Generation

```
Prompt: Google is
Generation: <s> Google is to deliver its latest Pixel phone to AT&T, instead of the original, cheaper Pixel 3a.
AT&T US is expected to unveil the Pixel 5a on the 8th of February, or in the evening of Friday the 12th next year.
However, it appears Google has announced the Pixel 5a as the second Pixel phone to be delivered to AT&T, rather than the original Pixel 3a.
According to a blog post published by Pixel US on 26th October, the Pixel 5a will be available on the 9th of March with a price of $599.99.
Google has also promised the Pixel 5a will be the first Pixel phone sold in the U.S., US.
In the announcement made the company is also hinting that the Pixel 5a will be available on Sprint's network, meaning the Pixel 5a won't be delivered to T-Mobile US.
A couple of months ago, the Google Pixel 5a, then known as the Pixel 4a, was revealed to the world on the 22nd of October alongside an invite for beta program for the Pixel 5a.
The Pixel 5a is believed to be the largest smartphone Pixel has ever produced.</s>
```

### Code Generation

```python
Prompt: This is a code snippet for sorting a list of number in Python language with python in-built function `sorted`:
"""python
number_list = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]

Generation: <s> This is a code snippet for sorting a list of number in Python language with python in-built function `sorted`:
"""python
number_list = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
print(sorted(number_list))
"""

[`sorted`](https://docs.python.org/2/library/ordereddict.html) is a function that helps you sort a list of values in Python language.
For the list of items like number, this function is also used to sort the list.
</s>
```

## ğŸ¤—Pretrained Models

æˆ‘ä»¬å·²ç»åœ¨Huggingfaceä¸Šå…¬å¼€å‘å¸ƒäº†æˆ‘ä»¬çš„æ£€æŸ¥ç‚¹ã€‚æ¬¢è¿æµ‹è¯•å’Œåˆ©ç”¨æˆ‘ä»¬çš„æ¨¡å‹ï¼


| Epoch | Step   | Pretrained tokens | Sequence Length | HuggingFace checkpoint |
| ----- | ------ | ----------------- | --------------- | ---------------------- |
| 1     | 25k    | 50B               | 2048            | [DataHammer/hammerllm-1.4b-25k](https://huggingface.co/DataHammer/hammerllm-1.4b-25k)                       |
| 1     | 50k    | 100B              | 2048            | [DataHammer/hammerllm-1.4b-50k](https://huggingface.co/DataHammer/hammerllm-1.4b-50k)                       |
| 1     | 75k    | 150B              | 2048            | [DataHammer/hammerllm-1.4b-75k](https://huggingface.co/DataHammer/hammerllm-1.4b-75k)                       |
| 1     | 100k   | 200B              | 2048            | [DataHammer/hammerllm-1.4b-100k](https://huggingface.co/DataHammer/hammerllm-1.4b-100k)                       |
| 1     | 125k   | 250B              | 2048            | [DataHammer/hammerllm-1.4b-125k](https://huggingface.co/DataHammer/hammerllm-1.4b-125k)                       |
| 1     | 150k   | 300B              | 2048            | [DataHammer/hammerllm-1.4b-150k](https://huggingface.co/DataHammer/hammerllm-1.4b-150k)                       |
| 1     | 172.5k | 345B              | 2048            | [DataHammer/hammerllm-1.4b-172k](https://huggingface.co/DataHammer/hammerllm-1.4b-172k)                       |
| 2     | 197k    | 395B       | 2048            | [DataHammer/hammerllm-1.4b-197k](https://huggingface.co/DataHammer/hammerllm-1.4b-197k)                       |
| 2     | 222k    | 445B      | 2048            | [DataHammer/hammerllm-1.4b-222k](https://huggingface.co/DataHammer/hammerllm-1.4b-222k)                       |

## ğŸ“ŠPerformance

### Perplexity on Latest Test Dataset

å¯¹äºæ²¡æœ‰è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„base modelï¼Œåˆ†æå®ƒä»¬åœ¨**é«˜æ—¶æ•ˆæ€§æµ‹è¯•æ•°æ®**ä¸Šçš„perplexityæ˜¯é‡è¦çš„ï¼Œè¿™å¯ä»¥åæ˜ äº†å®ƒä»¬åœ¨æ²¡æœ‰è®­ç»ƒæ•°æ®æ³„éœ²çš„æƒ…å†µä¸‹çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†æˆ‘ä»¬çš„æ¨¡å‹å’Œä¸€äº›è‘—åçš„sLLMåœ¨çš„ä¸­æ–‡å’Œè‹±æ–‡æœ€æ–°æµ‹è¯•é›†ä¸Šçš„perplexityã€‚

æˆ‘ä»¬çš„å®éªŒè¿‡ç¨‹å’Œè®¡ç®—éµå¾ª**Skywork**çš„è®¾ç½®ï¼Œæ‚¨å¯ä»¥å‚è€ƒ[è¿™ä¸ªé“¾æ¥](https://github.com/SkyworkAI/Skywork?tab=readme-ov-file#é¢†åŸŸæ•°æ®å›°æƒ‘åº¦è¯„ä¼°)ä»¥äº†è§£æ›´å¤šå®ç°çš„ç»†èŠ‚ã€‚

æˆ‘ä»¬çš„æ¨¡å‹åœ¨å›°æƒ‘åº¦ï¼ˆpplï¼‰æµ‹è¯•ä¸­çš„è¡¨ç°**ä»…æ¬¡äº**Qwen-1.5-1.8Bæ¨¡å‹ï¼Œä¼˜äºGemma-2Bã€InterLM2-1.8Bå’ŒMiniCPM-2Bï¼ˆ350kï¼‰ã€‚é‰´äºæˆ‘ä»¬çš„æ¨¡å‹å¤§å°è¾ƒå°ï¼ˆ1.4Bå°äº1.8Bï¼‰ï¼Œè¿™äº›å®éªŒç»“æœæ˜¯æœ‰æ„æ€çš„ã€‚


#### Chinese Corpus ([Skywork-ChineseDomainModelingEval](https://huggingface.co/datasets/Skywork/ChineseDomainModelingEval))

|                  | Tokens | Tech      | Movie     | Government | Game      | Finance  | General  | Avg.       |
| ---------------- | ------ | --------- | --------- | ---------- | --------- | -------- | -------- | ---------- |
| Gemma-2B         | 6T     | 71.92     | 271.95    | 93.16      | 122.59    | 46.53    | 74.93    | 92.92      |
| InterLM2-1.8B    | -      | 19.20     | 36.88     | 7.18       | 29.67     | 9.51     | 11.81    | 16.03      |
| MiniCPM-2B(350k) | -      | 17.32     | 36.44     | 7.00       | 26.59     | 7.92     | 10.26    | 14.56      |
| Qwen-1.5(1.8B)   | -      | 16.88     | **33.39** | 6.63       | **25.44** | 7.43     | 9.77     | **13.80**  |
| Qwen-1.5(0.5B)   | -      | 20.70     | 41.59     | 8.24       | 31.64     | 9.41     | 11.98    | 17.13      |
| Ours             | 50B    | 19.59     | 44.58     | 7.48       | 33.55     | 8.10     | 11.31    | 16.49      |
| Ours             | 100B   | 18.23     | 40.26     | 7.11       | 30.80     | 7.42     | 10.47    | 15.23      |
| Ours             | 150B   | 17.59     | 38.86     | 6.91       | 29.76     | 7.19     | 10.20    | 14.75      |
| Ours             | 200B   | 17.24     | 38.03     | 6.84       | 29.01     | 7.05     | 10.04    | 14.48      |
| Ours             | 250B   | 17.08     | 37.53     | 6.81       | 28.63     | 6.97     | 9.91     | 14.32      |
| Ours             | 300B   | 16.92     | 37.17     | 6.82       | 28.37     | 6.89     | 9.82     | 14.21      |
| Ours             | 345B   | 16.79     | 36.89     | 6.73       | 28.11     | 6.84     | 9.75     | 14.09      |
| Ours(epoch-2)    | 395B   | 16.71     | 36.64     | **6.60**   | 28.01     | 6.85     | 9.72     | 14.00      |
| Ours(epoch-2)    | 445B   | **16.67** | 36.43     | 6.68       | 27.86     | **6.75** | **9.65** | 13.95      |


#### English Corpus ([RealTimeData-Latest Collection](https://huggingface.co/collections/RealTimeData/latest-collection-65e1c5d70e180e6263f82589))

|                  | Tokens   | arxiv-latest | bbc-latest | github-latest | wikitext-latest | Avg.       |
| ---------------- | -------- | ------------ | ---------- | ------------- | --------------- | ---------- |
| TinyLlama-1.1B   | 3T       | 8.21         | **8.27**   | **5.90**      | **7.38**        | **7.37**   |
| Gemma-2B         | 6T       | 28.96        | 37.83      | 22.10         | 28.11           | 28.72      |
| Phi-1.5(1.3B)    | 30B \* 5 | 13.08        | 14.07      | 10.42         | 15.16           | 13.06      |
| InterLM2-1.8B    | -        | **7.84**     | 8.75       | 6.87          | 8.52            | 7.96       |
| MiniCPM-2B(350k) | -        | 10.81        | 9.15       | **nan**       | 8.08            | 9.28       |
| Qwen-1.5(1.8B)   | -        | 8.70         | 8.92       | **nan**       | 8.75            | 8.79       |
| Qwen-1.5(0.5B)   | -        | 10.42        | 10.93      | **nan**       | 10.58           | 10.64      |
| Ours             | 50B      | 10.67        | 13.19      | 9.14          | 10.47           | 10.77      |
| Ours             | 100B     | 9.87         | 11.97      | 8.46          | 9.71            | 9.93       |
| Ours             | 150B     | 9.57         | 11.63      | 8.16          | 9.38            | 9.61       |
| Ours             | 200B     | 9.41         | 11.43      | 8.06          | 9.21            | 9.45       |
| Ours             | 250B     | 9.39         | 11.21      | 7.96          | 9.13            | 9.35       |
| Ours             | 300B     | 9.79         | 11.24      | 7.90          | 9.03            | 9.41       |
| Ours             | 345B     | 9.59         | 11.11      | 7.86          | 9.04            | 9.33       |
| Ours(epoch-2)    | 395B     | 9.65         | 11.06      | 7.86          | 9.01            | 9.32       |
| Ours(epoch-2)    | 445B     | 9.34         | 11.07      | 7.86          | 9.04            | 9.26       |

**NOTE**: Average PPL of Qwen-1.5 and MiniCPM models have excluded GitHub dataset for NaN value.

### ğŸ““Tokenizer

ä¸ºäº†æ­ç¤ºæˆ‘ä»¬åˆ†è¯å™¨çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æŒ‡æ ‡å°†å…¶ä¸ä¸€äº›è‘—åå¼€æºLLMçš„åˆ†è¯å™¨è¿›è¡Œæ¯”è¾ƒï¼š
1. **å‹ç¼©ç‡ï¼š**æˆ‘ä»¬å°†åˆ†è¯å™¨çš„ä¸¤ç§å‹ç¼©ç‡ä¸ä¸€äº›å¼€æºLLMçš„åˆ†è¯å™¨è¿›è¡Œæ¯”è¾ƒï¼š
   * [Byte per tokenå‹ç¼©ç‡](https://kexue.fm/archives/9752#%E6%95%88%E6%9E%9C%E6%B5%8B%E8%AF%95)
   * [åŸºäºæ¯”è¾ƒçš„å‹ç¼©ç‡](https://arxiv.org/pdf/2309.16609.pdf)ï¼Œæµ‹é‡ä¸åŸºç¡€Llama-2-7Båˆ†è¯å™¨ç›¸æ¯”çš„ä¼˜åŠ¿ã€‚
   
2. **æ±‰å­—è¦†ç›–ç‡ï¼š**ä¸€ä¸ªå¥½çš„ä¸­æ–‡LLMåº”è¯¥è¦†ç›–æ›´å¤šçš„æ±‰å­—ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨[vocab-coverage](https://github.com/twang2218/vocab-coverage)æ¥è®¡ç®—æ±‰å­—çš„è¦†ç›–ç‡ï¼ŒåŒ…æ‹¬ï¼š
     * **ä¸€çº§æ±‰å­—è¦†ç›–ç‡**ï¼ˆFCCï¼‰åŒ…å«3500ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ±‰å­—ã€‚
     * **äºŒçº§æ±‰å­—è¦†ç›–ç‡**ï¼ˆSCCï¼‰åŒ…å«3000ä¸ªæ±‰å­—ã€‚
     * **ä¸‰çº§æ±‰å­—è¦†ç›–ç‡**ï¼ˆTCCï¼‰åŒ…å«1605ä¸ªä¸å¸¸è§çš„æ±‰å­—ã€‚

å®éªŒç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

|     Tokenizer      | FCC | SCC | TCC | Byte per Token $\uparrow$ | Comparied Compression $\downarrow$ |
|:------------------:|:-----------:|:-----------:|:-----------:|:-----------:|:----------------:|
| [Chatglm-6b](https://huggingface.co/THUDM/chatglm-6b)     |   99.97%    |   57.47%    |    2.99%    | 4.2911      |     0.5303     | 
| [Chatglm2/3-6b](https://huggingface.co/THUDM/chatglm2-6b)    |   **100.00%**  |   77.83%    |   13.89%    | 4.0329      |     0.5642     |  
| [Baichuan2-7b/14b](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat)  |   **100.00%**   |    99.8%    |   86.48%    | 4.1827      |     0.5440     | 
| [Internlm-7b/20b](https://huggingface.co/internlm/internlm-7b)   |   **100.00%**   |   65.93%    |    5.67%    | 4.3133      |     0.5276     |   
| [Qwen-7b/14b/72b](https://huggingface.co/Qwen/Qwen-7B)   |   **100.00%**   |   **100.00%**   |   **100.00%**   | 4.1326      |     0.5506     |   
| [Llama-2-7b/13b/70b](https://huggingface.co/meta-llama/Llama-2-7b-hf)  |   17.29%    |    0.13%    |    0.00%    | 2.2755      |        1.00         |  
| [Ours](https://github.com/Academic-Hammer/HammerLLM/tree/main/merge_tokenizer/internlm_merged_fast)   |    **100.00%**     |   **100.00%**    |    **100.00%**    |   **4.3143**   |     **0.5274**             | 

å®éªŒç»“æœæ­ç¤ºäº†æˆ‘ä»¬çš„åˆ†è¯å™¨åœ¨å‹ç¼©ç‡ï¼ˆå¯¹ä¸­æ–‡ã€è‹±æ–‡å’Œä»£ç æ•°æ®ï¼‰å’Œæ±‰å­—è¦†ç›–ç‡æ–¹é¢ï¼Œç›¸è¾ƒäºç°æœ‰æµè¡ŒLLMçš„åˆ†è¯å™¨çš„ä¼˜åŠ¿ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è§[REPRODUCE_zh-CN.md](./REPRODUCE_zh-CN.md).


### â©Acceleration


åœ¨ä»¥ä¸‹ä¸¤ç§ç­–ç•¥çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå®ç°per second per GPU **16k token**çš„é«˜ååé‡è¿›è¡Œè®­ç»ƒï¼š
- [Flash Attention 2](https://github.com/Dao-AILab/flash-attention)
- [torch.compile](https://pytorch.org/docs/stable/torch.compiler.html)

è®¡ç®—ååé‡çš„è®¾ç½®å¦‚ä¸‹ï¼š
* ZeRO-1
* block_size: 2048
* per_device_train_batch_size: 8
* gradient_accumulation_steps: 16


| Settings                          | tokens per GPU per second |
|-----------------------------------|---------------------------|
| None                              | CUDA OOM                  |
| Flash Attention 2                 | 13k                       |
| torch.compile                     | CUDA OOM                  |
| Flash Attention 2 + torch.compile | 16k                       |

ä¸ºäº†è¦†ç›–æ›´å¤šçš„æ±‰å­—ï¼Œæˆ‘ä»¬çš„åˆ†è¯å™¨æ¯”[TinyLlama](https://github.com/jzhang38/TinyLlama)å¤§å¾—å¤šï¼ˆ105789 > 32000ï¼‰ï¼Œå¯¼è‡´ååé‡ä½äºTinyLlamaï¼ˆ16k < 24kï¼‰ã€‚

ç„¶è€Œï¼Œå½“åˆ†è¯å™¨å¤§å°ç›¸åŒæ—¶ï¼Œæˆ‘ä»¬çš„ååé‡ä¸TinyLlamaç›¸å½“ï¼ˆä¸ºæ­¤å°†per_device_train_batch_sizeè®¾ç½®ä¸º20ï¼‰ã€‚


| Settings                          | tokens per GPU per second |
|-----------------------------------|---------------------------|
| Flash Attention 2 + torch.compile | 24k                       |

ä¸åˆ©ç”¨ä¸€äº›å¤æ‚æ“ä½œèåˆçš„TinyLlamaä¸åŒï¼Œæˆ‘ä»¬åªé€šè¿‡ç»“åˆ`torch.compile`å’Œ`flash attention 2`å³å¯å®ç°äº†è¿™ä¸€ååé‡ã€‚


## ğŸ“ˆTraining Progress

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»åœ¨è¶…è¿‡400Bçš„ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å­—ç¬¦å’Œä»£ç å­—ç¬¦ä¸Šä¼˜åŒ–äº†æˆ‘ä»¬çš„æ¨¡å‹ã€‚ä¸­æ–‡å’Œè‹±æ–‡æµ‹è¯•é›†çš„å›°æƒ‘åº¦ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°šæœªæ”¶æ•›ï¼Œå› æ­¤æˆ‘ä»¬ç›®å‰æ­£åœ¨ç»§ç»­é¢„è®­ç»ƒè¿‡ç¨‹ã€‚

æˆ‘ä»¬å°†å®šæœŸä¸Šä¼ å¹¶å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„checkpointï¼Œä»¥ä¾›å¼€æºç¤¾åŒºåˆ†æå’Œç ”ç©¶ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­**æŸå¤±**çš„å˜åŒ–è¶‹åŠ¿åœ¨ä¸‹å›¾ä¸­æ˜¾ç¤ºï¼š
![Training Loss](.github/images/train_loss.jpg)

åŸºäºSkyworkç³»åˆ—æ¨¡å‹ï¼Œåœ¨**Skywork-ChineseDomainModelingEval**å’Œ**RealTimeData-Latest Collection**æ•°æ®é›†ä¸Šçš„æ ‡å‡†åŒ–åçš„**perplexity**å˜åŒ–è¶‹åŠ¿å¦‚ä¸‹ï¼š

<table rules="none" align="center">
	<tr>
		<td>
			<center>
				<img src=".github/images/zh-ppl.jpg" />
				<br/>
				<center>ä¸­æ–‡ Perplexity</center>
      </center>
		</td>
		<td>
			<center>
				<img src=".github/images/en-ppl.jpg" />
        <br/>
				<center>è‹±æ–‡ Perplexity</center>
			</center>
		</td>
	</tr>
</table>

## â™»ï¸Reproduce Our Work

æˆ‘ä»¬çš„è´¡çŒ®ä¹‹ä¸€åœ¨äºå…¬å¼€äº†ä»å¤´å®Œæˆé¢„è®­ç»ƒçš„å®Œæ•´æµç¨‹ã€‚ä½ å¯ä»¥åœ¨[è¯¥æ–‡ä»¶](./REPRODUCE_zh-CN.md)ä¸­æ‰¾åˆ°æ‰€æœ‰çš„ä¿¡æ¯ã€‚

## ğŸ–Šï¸Citing

```
@software{Ziao_HammerLLM_2024,
author = {Ziao, Ma and Tian, Lan and Yang, Yizhe and Yong, Hu},
month = apr,
title = {{HammerLLM}},
url = {https://github.com/Academic-Hammer/HammerLLM},
version = {1.0.0},
year = {2024}
}
```

## ğŸ¤Contact Us

æ‚¨å¯ä»¥é€šè¿‡å¦‚ä¸‹é‚®ç®±è”ç³»åˆ°æˆ‘ä»¬: 
* maziaoylwt@gmail.com
* lantiangmftby@gmail.com


## ğŸ’³License


æœ¬é¡¹ç›®åŸºäº MIT å¼€æºåè®®[license](./LICENSE).

## ğŸ’ªContributing

<!-- Guidelines for how others can contribute to this repository, including coding standards, pull requests, and other relevant information. -->

æˆ‘ä»¬å¸Œæœ›è¿™ä»½æ–‡æ¡£å¯ä»¥å¯¹æ‚¨ç†è§£å’Œä½¿ç”¨æˆ‘ä»¬çš„HammerLLMæ¨¡å‹æœ‰æ‰€å¸®åŠ©ã€‚å¦‚æœ‰è¿›ä¸€æ­¥çš„é—®é¢˜æˆ–éœ€è¦æ”¯æŒï¼Œè¯·åœ¨æ­¤ä»“åº“ä¸­æäº¤issueã€‚
