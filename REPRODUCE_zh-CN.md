<div align="center">
</div>

# HammerLLMğŸ”¨

ä¸­æ–‡ | [English](REPRODUCE.md)

## Table of Contents

- [HammerLLMğŸ”¨](#hammerllm)
  - [Table of Contents](#table-of-contents)
  - [âœ¨Getting Start](#getting-start)
    - [ğŸ““Build Tokenizer](#build-tokenizer)
      - [Experimental Results](#experimental-results)
    - [âš™ï¸Train Model](#ï¸train-model)
      - [âš’Architecture](#architecture)
      - [âš™Training](#training)
      - [ğŸ—‚Training Corpus](#training-corpus)
      - [â©Acceleration Strategy](#acceleration-strategy)
      - [ğŸ–¥Hardware Requirement](#hardware-requirement)
      - [Pretrain](#pretrain)
        - [Environment Setup](#environment-setup)
        - [Data Preparation](#data-preparation)
        - [Start Training (train.sh)](#start-training-trainsh)
      - [Convert Checkpoints](#convert-checkpoints)
      - [Inference](#inference)
        - [Example Code](#example-code)

## âœ¨Getting Start

### ğŸ““Build Tokenizer

è¯¥ç ”ç©¶çš„åŠ¨æœºæ˜¯æ„å»ºä¸€ä¸ªåˆ†è¯å™¨ï¼Œè¯¥åˆ†è¯å™¨åœ¨ä¸­æ–‡ã€è‹±æ–‡å’Œä»£ç æ•°æ®ä¸Šå…·æœ‰æ›´é«˜çš„å‹ç¼©ç‡ï¼Œå¹¶è¦†ç›–100%çš„æ±‰å­—ã€‚é‰´äºInternLMåˆ†è¯å™¨è¾ƒé«˜çš„å‹ç¼©ç‡ï¼Œæˆ‘ä»¬å†³å®šå¯¹[InternLMåˆ†è¯å™¨](https://huggingface.co/internlm/internlm-7b)è¿›è¡Œäº†ä»¥ä¸‹æ”¹è¿›ï¼š
* è¡¥å……100%çš„å¸¸è§æ±‰å­—ä»¥è®­ç»ƒæˆ‘ä»¬è‡ªå·±çš„åˆ†è¯å™¨
* æ ¹æ®InternLMåˆ†è¯å™¨æ ¡å‡†æˆ‘ä»¬åˆ†è¯å™¨çš„score
* è¡¥å……InternLMåˆ†è¯å™¨ä¸­æ²¡æœ‰è€Œæˆ‘ä»¬è®­ç»ƒçš„åˆ†è¯å™¨ä¸­æœ‰çš„tokenï¼Œå¦‚ä¸€äº›å¸¸è§æ±‰å­—å’Œç”¨æˆ·å®šä¹‰çš„ç¬¦å·
* è½¬æ¢æˆLlama formatçš„åˆ†è¯å™¨
æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](./merge_tokenizer)æ‰¾åˆ°æ›´å¤šå…³äºæˆ‘ä»¬å¦‚ä½•æ„å»ºåˆ†è¯å™¨çš„è¯¦ç»†ä¿¡æ¯ã€‚
æˆ‘ä»¬çš„åˆ†è¯å™¨ä½äº[è¿™é‡Œ](https://github.com/Academic-Hammer/HammerLLM/tree/main/merge_tokenizer/internlm_merged_fast)ï¼Œå®ƒåŒ…å«`105789`ä¸ªæ ‡è®°ï¼Œå®ç°äº†`100%`çš„æ±‰å­—è¦†ç›–ã€‚

 
#### Experimental Results

ä¸ºäº†æ­ç¤ºæˆ‘ä»¬åˆ†è¯å™¨çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æŒ‡æ ‡å°†å…¶ä¸ä¸€äº›è‘—åå¼€æºLLMçš„åˆ†è¯å™¨è¿›è¡Œæ¯”è¾ƒï¼š
1. **å‹ç¼©ç‡ï¼š**æˆ‘ä»¬å°†åˆ†è¯å™¨çš„ä¸¤ç§å‹ç¼©ç‡ä¸ä¸€äº›å¼€æºLLMçš„åˆ†è¯å™¨è¿›è¡Œæ¯”è¾ƒï¼š
   * [æ¯æ ‡è®°çš„å­—èŠ‚å‹ç¼©ç‡](https://kexue.fm/archives/9752#%E6%95%88%E6%9E%9C%E6%B5%8B%E8%AF%95)
   * [æ¯”è¾ƒå‹ç¼©](https://arxiv.org/pdf/2309.16609.pdf)ï¼Œæµ‹é‡ä¸åŸºç¡€Llama-2-7Båˆ†è¯å™¨ç›¸æ¯”çš„ä¼˜åŠ¿ã€‚
   
   è¯·å‚è€ƒè¿™ä¸ª[pthonè„šæœ¬](tokenizer/analyze_tokenizer/compression_analyze.py)è·å–æ›´å¤šç»†èŠ‚ã€‚
   æˆ‘ä»¬åœ¨ä¸­æ–‡ã€è‹±æ–‡å’Œä»£ç æµ‹è¯•é›†ä¸Šè¯„ä¼°åˆ†è¯å™¨ä»¥è®¡ç®—å‹ç¼©ç‡ï¼Œæ•°æ®æ¥æºå¦‚ä¸‹ï¼š
   * ä¸­æ–‡ï¼š[Skywork/ChineseDomainModelingEval](https://huggingface.co/datasets/Skywork/ChineseDomainModelingEval)
   * è‹±æ–‡ï¼š[EleutherAI/pile](https://huggingface.co/datasets/EleutherAI/pile)çš„æµ‹è¯•é›†
   * ä»£ç ï¼šæ¥è‡ª[Pile-GitHub](https://huggingface.co/datasets/EleutherAI/pile)çš„åˆ†å‰²
   
   è¿™äº›æµ‹è¯•æ•°æ®åœ¨[è¿™ä¸ªé“¾æ¥](https://huggingface.co/datasets/DataHammer/Tokenizer-Test-Set/tree/main)ä¸Šå…¬å¼€å¯ç”¨ã€‚
2. **æ±‰å­—è¦†ç›–ç‡ï¼š**ä¸€ä¸ªå¥½çš„ä¸­æ–‡LLMåº”è¯¥è¦†ç›–æ›´å¤šçš„æ±‰å­—ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨[vocab-coverage](https://github.com/twang2218/vocab-coverage)æ¥è®¡ç®—æ±‰å­—çš„è¦†ç›–ç‡ï¼ŒåŒ…æ‹¬ï¼š
     * **ä¸€çº§æ±‰å­—è¦†ç›–ç‡**ï¼ˆFCCï¼‰åŒ…å«3500ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ±‰å­—ã€‚
     * **äºŒçº§æ±‰å­—è¦†ç›–ç‡**ï¼ˆSCCï¼‰åŒ…å«3000ä¸ªæ±‰å­—ã€‚
     * **ä¸‰çº§æ±‰å­—è¦†ç›–ç‡**ï¼ˆTCCï¼‰åŒ…å«1605ä¸ªä¸å¸¸è§çš„æ±‰å­—ã€‚
     
ä¸ºäº†è·å¾—å®éªŒç»“æœï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```shell
cd analyze_tokenizer
# compute the compression rate
python compression_analyze.py
# compute the Chinese character coverage
python coverage_analyze.py
```

å®éªŒç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

|     Tokenizer      | FCC | SCC | TCC | Byte per Token $\uparrow$ | Comparied Compression $\downarrow$ |
|:------------------:|:-----------:|:-----------:|:-----------:|:-----------:|:----------------:|
| [Chatglm-6b](https://huggingface.co/THUDM/chatglm-6b)     |   99.97%    |   57.47%    |    2.99%    | 4.2911      |     0.5303     | 
| [Chatglm2/3-6b](https://huggingface.co/THUDM/chatglm2-6b)    |   **100.00%**  |   77.83%    |   13.89%    | 4.0329      |     0.5642     |  
| [Baichuan2-7b/14b](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat)  |   **100.00%**   |    99.8%    |   86.48%    | 4.1827      |     0.5440     | 
| [Internlm-7b/20b](https://huggingface.co/internlm/internlm-7b)   |   **100.00%**   |   65.93%    |    5.67%    | 4.3133      |     0.5276     |   
| [Qwen-7b/14b/72b](https://huggingface.co/Qwen/Qwen-7B)   |   **100.00%**   |   **100.00%**   |   **100.00%**   | 4.1326      |     0.5506     |   
| [Llama-2-7b/13b/70b](https://huggingface.co/meta-llama/Llama-2-7b-hf)  |   17.29%    |    0.13%    |    0.00%    | 2.2755      |        1.00         |  
| [Ours](https://github.com/Academic-Hammer/HammerLLM/tree/main/merge_tokenizer/internlm_merged_fast)   |    **100.00%**     |   **100.00%**    |    **100.00%**    |   **4.3143**   |     **0.5274**             | 

å®éªŒç»“æœæ­ç¤ºäº†æˆ‘ä»¬çš„åˆ†è¯å™¨åœ¨å‹ç¼©ç‡ï¼ˆå¯¹ä¸­æ–‡ã€è‹±æ–‡å’Œä»£ç æ•°æ®ï¼‰å’Œæ±‰å­—è¦†ç›–ç‡æ–¹é¢ï¼Œç›¸è¾ƒäºç°æœ‰æµè¡ŒLLMçš„åˆ†è¯å™¨çš„ä¼˜åŠ¿ã€‚

### âš™ï¸Train Model

#### âš’Architecture

æˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨äº†åŸºç¡€çš„Llamaæ¶æ„ï¼Œä»¥ä¸‹æ˜¯å…³é”®å‚æ•°ï¼š


| Setting             | Description             |
| ------------------- | ----------------------- |
| parameters          | 1.4B                    |
| attention           | Grouped-query Attention (GQA) |
| num layers          | 22                      |
| num attention heads | 32                      |
| query groups        | 4                       |
| hidden size         | 2048                    |
| intermediate size   | 5632                    |
| activation func     | SiLU                    |
| max sequence length | 2048                    |

#### âš™Training Hyper-parameter

æˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹ç»è¿‡ç²¾å¿ƒè§„åˆ’å’Œæ‰§è¡Œï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„å¥å£®æ€§å’Œçµæ´»æ€§ï¼š

| Setting                     | Description |
| --------------------------- | ----------- |
| block size                  | 2048        |
| per-device batch size       | 8           |
| gradient accumulation steps | 16          |
| num device                  | 8           |
| total batch size            | 2M tokens   |
| max learning rate           | 5e-4        |
| min learning rate           | 5e-5        |
| warmup steps                | 2000        |
| learning rate schedule      | cosine      |

æˆ‘ä»¬ä¿®æ”¹äº†å·²æœ‰çš„learning schedulerï¼Œæ›´å¤šç»†èŠ‚å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/huggingface/transformers/issues/28441)æ‰¾åˆ°ã€‚


#### ğŸ—‚Training Corpus

æˆ‘ä»¬çš„æ¨¡å‹åœ¨ç²¾å¿ƒæŒ‘é€‰çš„**ä¸­æ–‡ã€è‹±æ–‡å’Œç¼–ç¨‹**æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨åŸ¹å…»å¹¿æ³›çš„ä¸­è‹±æ–‡è¯­è¨€ç†è§£èƒ½åŠ›ï¼š

| Dataset                                                                   | Split   | Token (Billion) | Domain  |
| ------------------------------------------------------------------------- | ------- | --------------- | ------- | 
| [ChineseWebText](https://huggingface.co/datasets/CASIA-LM/ChineseWebText) | Chinese | 142             | Chinese |
| [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)    | English | 128             | English | 
| [Pile-arXiv](https://huggingface.co/datasets/EleutherAI/pile)             | English | 38              | English | 
| [Pile-Wiki](https://huggingface.co/datasets/EleutherAI/pile)              | English | 12              | English | 
| [Pile-GitHub](https://huggingface.co/datasets/EleutherAI/pile)            | Code    | 30              | Coding  |

#### â©Acceleration Strategy

åœ¨ä»¥ä¸‹ä¸¤ç§ç­–ç•¥çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå®ç°per second per GPU **16k token**çš„é«˜ååé‡è¿›è¡Œè®­ç»ƒï¼š
- [Flash Attention 2](https://github.com/Dao-AILab/flash-attention)
- [torch.compile](https://pytorch.org/docs/stable/torch.compiler.html)

è®¡ç®—ååé‡çš„è®¾ç½®å¦‚ä¸‹ï¼š
* ZeRO-1
* block_size: 2048
* per_device_train_batch_size: 8
* gradient_accumulation_steps: 16


| Settings                          | tokens per GPU per second |
|-----------------------------------|---------------------------|
| None                              | CUDA OOM                  |
| Flash Attention 2                 | 13k                       |
| torch.compile                     | CUDA OOM                  |
| Flash Attention 2 + torch.compile | 16k                       |

ä¸ºäº†è¦†ç›–æ›´å¤šçš„æ±‰å­—ï¼Œæˆ‘ä»¬çš„åˆ†è¯å™¨æ¯”[TinyLlama](https://github.com/jzhang38/TinyLlama)å¤§å¾—å¤šï¼ˆ105789 > 32000ï¼‰ï¼Œå¯¼è‡´ååé‡ä½äºTinyLlamaï¼ˆ16k < 24kï¼‰ã€‚

ç„¶è€Œï¼Œå½“åˆ†è¯å™¨å¤§å°ç›¸åŒæ—¶ï¼Œæˆ‘ä»¬çš„ååé‡ä¸TinyLlamaç›¸å½“ï¼ˆä¸ºæ­¤å°†per_device_train_batch_sizeè®¾ç½®ä¸º20ï¼‰ã€‚


| Settings                          | tokens per GPU per second |
|-----------------------------------|---------------------------|
| Flash Attention 2 + torch.compile | 24k                       |

ä¸åˆ©ç”¨ä¸€äº›å¤æ‚æ“ä½œèåˆçš„TinyLlamaä¸åŒï¼Œæˆ‘ä»¬åªé€šè¿‡ç»“åˆ`torch.compile`å’Œ`flash attention 2`å³å¯å®ç°äº†è¿™ä¸€ååé‡ã€‚


#### ğŸ–¥Hardware Requirement

æˆ‘ä»¬çš„é¢„è®­ç»ƒæ˜¯åœ¨é…å¤‡1TB CPUå†…å­˜çš„8x80G A100æœåŠ¡å™¨ä¸Šè¿è¡Œçš„ã€‚
æ‚¨å¯ä»¥ä½¿ç”¨è¾ƒå°‘çš„GPUå¡å’Œå†…å­˜ï¼Œé€šè¿‡å‡å°æ‰¹é‡å¤§å°æ¥é€‚é…æ‚¨çš„ç¡¬ä»¶æ¡ä»¶ã€‚

#### Pretrain

##### Environment Setup

æˆ‘ä»¬å·²ç»ä¸ºé¢„è®­ç»ƒå‡†å¤‡äº†Dockerç¯å¢ƒï¼Œè¯¥ç¯å¢ƒå·²ç»é›†æˆäº†`flash attention 2`å’Œ`torch.compile`ä»¥å®ç°é«˜æ•ˆçš„é¢„è®­ç»ƒã€‚Dockeræ–‡ä»¶ä½äº[è¿™é‡Œ](./Dockerfile)ã€‚


##### Data Preparation

è¯·å‚è€ƒæˆ‘ä»¬çš„[æ•°æ®å‡†å¤‡æŒ‡å—](./data_process/README.md)ï¼Œäº†è§£æ›´å¤šå…³äºæˆ‘ä»¬å¦‚ä½•é¢„å¤„ç†é¢„è®­ç»ƒæ•°æ®é›†çš„ç»†èŠ‚ï¼Œä¾‹å¦‚æ•°æ®é›†çš„é‡æ–°æ ¼å¼åŒ–å’Œå……åˆ†çš„æ´—ç‰Œã€‚


##### Start Training ([train.sh](./pretrain/train.sh))

ä¸€æ—¦æ‚¨å‡†å¤‡å¥½äº†è¿è¡Œç¯å¢ƒå’Œæ•°æ®ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯åŠ¨é¢„è®­ç»ƒï¼š

```shell
set -ex
export WANDB_PROJECT=hammerllm
BASE_DIR="$PWD"
DATE=$(TZ=Asia/Shanghai date +'%Y%m%d%H%M%S')
CONFIG_PATH=${BASE_DIR}/configs/hammerllm
RUN_NAME=hammerllm_torch_compile_flash_attn_2
OUTPUT_DIR=${BASE_DIR}/checkpoint/${RUN_NAME}

DATA_SEED=3407
MODEL_SEED=3407

export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export TOKENIZERS_PARALLELISM=false
export WANDB_MODE=online

if [ ! -d ${OUTPUT_DIR} ]
then
  mkdir -p ${OUTPUT_DIR}
fi
echo "Setting checkpoint directory to ${OUTPUT_DIR}"

MASTER_PORT=$(shuf -n 1 -i 60000-65535)
torchrun --nproc_per_node=8 --master_port ${MASTER_PORT} train.py \
  --model_name_or_path ${CONFIG_PATH} \
  --use_flash_attention_2 \
  --use_torch_compile \
  --train_file /path/to/your/tokenized/train/dataset \
  --validation_files /path/to/your/tokenized/validation/dataset_1 /path/to/your/tokenized/validation/dataset_2 ... \
  --preprocessing_num_workers 100 \
  --block_size 2048 \
  --do_train \
  --do_eval \
  --per_device_train_batch_size 8 \
  --per_device_eval_batch_size 8 \
  --gradient_accumulation_steps 16 \
  --logging_steps 10 \
  --max_steps 1000000 \
  --warmup_steps 2000 \
  --eval_steps 500 \
  --save_steps 500 \
  --evaluation_strategy steps \
  --save_strategy steps \
  --greater_is_better false \
  --load_best_model_at_end false \
  --ddp_find_unused_parameters false \
  --remove_unused_columns false \
  --save_total_limit 50 \
  --learning_rate 5e-4 \
  --lr_scheduler_type cosine \
  --output_dir ${OUTPUT_DIR} \
  --report wandb \
  --run_name ${RUN_NAME} \
  --bf16 \
  --seed ${MODEL_SEED} \
  --data_seed ${DATA_SEED} \
  --deepspeed ${BASE_DIR}/configs/zero_1.json

```

ä¸Šè¿°shellä»£ç æ˜¯ç”¨äºé¢„è®­ç»ƒçš„å¯åŠ¨è„šæœ¬ï¼Œå…³äºæˆ‘ä»¬é¢„è®­ç»ƒä»£ç åº“çš„è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/tiny-llm/TrainerCodeBase/blob/main/readme.md)æ‰¾åˆ°ã€‚


#### Convert Checkpoints

checkpointä¸­åŒ…å«ç”±`torch.compile`å¼•å…¥çš„ä¸€äº›ä¸å¯»å¸¸çš„key nameï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹å‚æ•°åŠ è½½å¤±è´¥ã€‚æˆ‘ä»¬å·²ç»åœ¨[è¿™é‡Œ](pretrain/convert_checkpoint.py)æä¾›äº†è½¬æ¢è„šæœ¬ï¼Œå¯¹è¿™äº›key nameè¿›è¡Œæ ¡å‡†ã€‚


```shell
python convert_checkpoint.py --input-path <path of saved checkpoint> --output-path <path of converted transformers checkpoint>
```

#### Inference

##### Example Code
å¦‚ä¸‹ä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨**HuggingFace** `transformers`ä¸æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œäº¤äº’ï¼š


```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = 'DataHammer/hammerllm-1.4b-222k'
text = 'åŒ—äº¬ç†å·¥å¤§å­¦æ˜¯'
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
# if your device donot support the bfloat16, you could remove it
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)

input_ids = tokenizer(text, return_tensors='pt').input_ids
output = model.generate(
    input_ids=input_ids.cuda(),
    max_length=min(int(len(input_ids) + 100), 1024),
    do_sample=True,
    top_p=0.95
).tolist()

generation = tokenizer.decode(output[0])
print(generation)
```
