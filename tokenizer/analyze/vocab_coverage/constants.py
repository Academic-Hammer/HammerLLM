# -*- coding: utf-8 -*-
import os

# Embedding 的位置
EMBEDDING_POSITION_INPUT = 'input'
EMBEDDING_POSITION_OUTPUT = 'output'
EMBEDDING_POSITION_ALL = [
    EMBEDDING_POSITION_INPUT,
    EMBEDDING_POSITION_OUTPUT,
]

# 字符集
## 中文字符集
CHARSET_CHINESE_GSCC_1 = '《通用规范汉字表》一级汉字'
CHARSET_CHINESE_GSCC_2 = '《通用规范汉字表》二级汉字'
CHARSET_CHINESE_GSCC_3 = '《通用规范汉字表》三级汉字'
CHARSET_CHINESE_SFCNC_A = '《常用國字標準字體表》甲表'
CHARSET_CHINESE_SFCNC_B = '《常用國字標準字體表》乙表'
CHARSET_CHINESE_UNICODE = '《Unicode中日韩统一表意文字》'
CHARSET_EXT = '(增)'
CHARSET_CHINESE_SFCNC_A_EXT = CHARSET_CHINESE_SFCNC_A + CHARSET_EXT
CHARSET_CHINESE_SFCNC_B_EXT = CHARSET_CHINESE_SFCNC_B + CHARSET_EXT
CHARSET_CHINESE_UNICODE_EXT = CHARSET_CHINESE_UNICODE + CHARSET_EXT
CHARSET_CHINESE_COMMON = '汉字(常用字)'
CHARSET_CHINESE_RARE = '汉字(生僻字)'
CHARSET_CHINESE = '汉字'
CHARSET_CHINESE_SETS = [
    CHARSET_CHINESE_GSCC_1,
    CHARSET_CHINESE_GSCC_2,
    CHARSET_CHINESE_GSCC_3,
    CHARSET_CHINESE_SFCNC_A_EXT,
    CHARSET_CHINESE_SFCNC_B_EXT,
    CHARSET_CHINESE_UNICODE_EXT,
]
CHARSET_CHINESE_SIMPLIFIED_SETS = [
    CHARSET_CHINESE_COMMON,
    CHARSET_CHINESE_RARE,
]
## 非中文字符集
CHARSET_JAPANESE = '日文'
CHARSET_KOREAN = '韩文'
CHARSET_ENGLISH = '字母'
CHARSET_DIGIT = '数字'
CHARSET_OTHER = '其他'

CHARSET_TOKEN_SETS = CHARSET_CHINESE_SIMPLIFIED_SETS + [
    CHARSET_JAPANESE,
    CHARSET_KOREAN,
    CHARSET_ENGLISH,
    CHARSET_DIGIT,
    CHARSET_OTHER,
]

CHARSET_CJK_SETS = CHARSET_CHINESE_SETS + [
    CHARSET_JAPANESE,
    CHARSET_KOREAN,
]

CHARSET_CHARACTER_SETS = CHARSET_CHINESE_SETS

# 颜色
PALETTE_CHINESE_GSCC_1 = '#B04759'
PALETTE_CHINESE_GSCC_2 = '#E76161'
PALETTE_CHINESE_GSCC_3 = '#F99B7D'
PALETTE_CHINESE_SFCNC_A = '#146C94'
PALETTE_CHINESE_SFCNC_B = '#19A7CE'
PALETTE_CHINESE_UNICODE = '#E893CF'
PALETTE_CHINESE_COMMON = '#D50000'
PALETTE_CHINESE_RARE = '#7B1FA2'
PALETTE_JAPANESE = '#827717'
PALETTE_KOREAN = '#FFA000'
PALETTE_ENGLISH = '#2E7D32'
PALETTE_DIGIT = '#01579B'
PALETTE_OTHER = '#212121'

PALETTE_WORD_IT = '#FF5722'
PALETTE_WORD_FINANCE = '#FF9800'
PALETTE_WORD_IDIOM = '#FFC107'
PALETTE_WORD_PLACE = '#FFEB3B'
PALETTE_WORD_PERSON = '#CDDC39'
PALETTE_WORD_POEM = '#8BC34A'
PALETTE_WORD_MEDICAL = '#4CAF50'
PALETTE_WORD_FOOD = '#009688'
PALETTE_WORD_LAW = '#00BCD4'
PALETTE_WORD_CAR = '#03A9F4'
PALETTE_WORD_ANIMAL = '#2196F3'

PALETTE_REGION_BACKGROUND = "#EEEEEE"
PALETTE_TEXT = "#000000"
PALETTE_BACKGROUND = "#FFFFFF"


# 颗粒度
GRANULARITY_TOKEN = 'token'
GRANULARITY_CHARACTER = 'char'
GRANULARITY_WORD = 'word'
GRANULARITY_SENTENCE = 'sentence'
GRANULARITY_PARAGRAPH = 'paragraph'

GRANULARITY_SETS = [
    GRANULARITY_TOKEN,
    GRANULARITY_CHARACTER,
    GRANULARITY_WORD,
    GRANULARITY_SENTENCE,
    GRANULARITY_PARAGRAPH,
]

# 目录结构

FOLDER_IMAGES = 'images'
FOLDER_ASSETS = 'assets'
FOLDER_COVERAGE = 'coverage'
FOLDER_EMBEDDING = 'embedding'
FOLDER_FULLSIZE = 'fullsize'
FOLDER_THUMBNAIL = 'thumbnail'

FOLDER_IMAGES_ASSETS_COVERAGE = os.path.join(FOLDER_IMAGES, FOLDER_ASSETS, FOLDER_COVERAGE)
FOLDER_IMAGES_ASSETS_EMBEDDING = os.path.join(FOLDER_IMAGES, FOLDER_ASSETS, FOLDER_EMBEDDING)
FOLDER_IMAGES_THUMBNAIL = os.path.join(FOLDER_IMAGES, FOLDER_THUMBNAIL)
FOLDER_IMAGES_FULLSIZE = os.path.join(FOLDER_IMAGES, FOLDER_FULLSIZE)

# 降维方法

REDUCER_TSNE = 'tsne'
REDUCER_TSNE_CUML = 'tsne_cuml'
REDUCER_UMAP = 'umap'
REDUCER_UMAP_CUML = 'umap_cuml'
REDUCER_UMAP_TSNE = 'umap_tsne'
REDUCER_UMAP_TSNE_CUML = 'umap_tsne_cuml'
REDUCER_UMAP_TSNE_CUML_BOTH = 'umap_tsne_cuml_both'

REDUCER_SET = [
    REDUCER_TSNE,
    REDUCER_TSNE_CUML,
    REDUCER_UMAP,
    REDUCER_UMAP_CUML,
    REDUCER_UMAP_TSNE,
    REDUCER_UMAP_TSNE_CUML,
    REDUCER_UMAP_TSNE_CUML_BOTH,
]

# 特殊字符
TEXT_LEADING_UNDERSCORE = '▁'
TEXT_OPENAI_END_OF_TEXT = '<|endoftext|>'

# 文件
FOLDER_CACHE = '.cache'
FILE_CACHE = 'embedding.cache'
FILE_CACHE_LOCK = 'embedding.cache.lock'
FILE_CHARSET_CHAR = 'charsets_char.json'
FILE_CHARSET_TOKEN = 'charsets_token.json'
FILE_DICT_WORD = 'dict_word.json'
FILE_DATASET_SENTENCE = 'dataset_sentence_short.json'
FILE_DATASET_PARAGRAPH = 'dataset_sentence_long.json'
FILE_CHARSET_DICT = {
    GRANULARITY_TOKEN: FILE_CHARSET_TOKEN,
    GRANULARITY_CHARACTER: FILE_CHARSET_CHAR,
    GRANULARITY_WORD: FILE_DICT_WORD,
    GRANULARITY_SENTENCE: FILE_DATASET_SENTENCE,
    GRANULARITY_PARAGRAPH: FILE_DATASET_PARAGRAPH,
}
FILE_MODELS = 'models.yaml'
FILE_MODELS_README = 'models_readme.yaml'
FILE_README_MD = 'README.md'
FILE_README_TEMPLATE_MD = 'README.template.md'
FILE_GRAPHS_MD = 'graphs.md'

# 模型名称模式

PATTERN_MODEL_NAME_LLAMA = ['llama', 'vicuna', 'alpaca', 'koala', 'wizardlm', 'guanaco', 'atom']
PATTERN_MODEL_NAME_OPENAI = ['openai']
PATTERN_MODEL_NAME_QWEN = ['qwen']
PATTERN_MODEL_NAME_BBPE = ["6b", "7b", "12b", "13b", "llama", "moss", "gpt", "openai", "bloom"]
PATTERN_TOKENIZER_BBPE = [
        "RobertaTokenizer",
        "BartTokenizer",
        "BloomTokenizer",
        "LlamaTokenizer",
        "GPT2Tokenizer",
        "GPTNeoXTokenizer",
        "DebertaTokenizer",
        "OpenAIGPTTokenizer",
        "BartTokenizer",
        "tiktoken.core.Encoding",
    ]
PATTERN_MODEL_NAME_INT8 = ['12b', '13b', 'taiwan-llama', 'moss', 'atomgpt']
PATTERN_MODEL_NAME_FP16 = ['6b', '7b', 'llama', 'gpt']
PATTERN_MODEL_NAME_CAUSALLM = ['falcon', 'aquila', 'qwen', 'baichuan', 'mpt']

# Markdown Style
MARKDOWN_STYLE_SINGLE_LINE = 'single_line'
MARKDOWN_STYLE_DOUBLE_LINE = 'double_line'
