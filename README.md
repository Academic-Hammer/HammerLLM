<div align="center">

# HammerLLMğŸ”¨

English | [ä¸­æ–‡](README_zh-CN.md)

</div>

<h5 align=center>

[![hf](https://img.shields.io/badge/ğŸ¤—-Hugging%20Face-blue.svg)](https://huggingface.co/collections/DataHammer/hammerllm-14b-660d227bf2e7fcbf6ceb5620)
[![License](https://img.shields.io/badge/Code%20License-MIT-yellow)](https://github.com/Academic-Hammer/HammerLLM/blob/main/LICENSE)
[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FAcademic-Hammer%2FHammerLLM&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=Visitor&edge_flat=false)](https://hits.seeyoufarm.com)
</h5>

Welcome to the pre-training repository for our **s**mall-size **L**arge **L**anguage **M**odel (sLLM) with **1.4 billion** parameters, leveraged on the **Llama 2** architecture. Our sLLM exhibits advantages in following features:
1. Llama-2 Compatible LLM for Chinese and EnglishğŸ¦™
2. Simple yet Efficient Training CodebaseğŸš€
3. Complete Open-Source: model weights, environment, codebase, and hyper-parametersğŸ”—
4. Comparable performance with advanced sLLMs with the similar model scaleğŸ¥‡
5. Tokenizer with highest compression rate and 100% Chinese character coverageğŸ†


## ğŸ”¥What's New!

* **[2024.4.2]** ğŸ‰ğŸ‰ğŸ‰Release all the resources about our HammerLLMğŸ”¨, including model weights, docker environment, and training codebase

## ğŸ§¾ Next

- [ ] Continue to optimize our models on more Chinese, English, and Code tokens
- [ ] Pretrain on more Code data

## Table of Contents

- [HammerLLMğŸ”¨](#hammerllm)
  - [ğŸ”¥What's New!](#whats-new)
  - [ğŸ§¾ Next](#-next)
  - [Table of Contents](#table-of-contents)
  - [âš™ï¸Inference](#ï¸inference)
    - [Example Code](#example-code)
  - [ğŸ™ŒCases](#cases)
    - [Chinese Generation](#chinese-generation)
    - [English Generation](#english-generation)
    - [Code Generation](#code-generation)
  - [ğŸ¤—Pretrained Models](#pretrained-models)
  - [ğŸ“ŠPerformance](#performance)
    - [Perplexity on Latest Test Dataset](#perplexity-on-latest-test-dataset)
      - [Chinese Corpus (Skywork-ChineseDomainModelingEval)](#chinese-corpus-skywork-chinesedomainmodelingeval)
      - [English Corpus (RealTimeData-Latest Collection)](#english-corpus-realtimedata-latest-collection)
    - [ğŸ““Tokenizer](#tokenizer)
    - [â©Acceleration](#acceleration)
  - [ğŸ“ˆTraining Progress](#training-progress)
  - [â™»ï¸Reproduce Our Work](#ï¸reproduce-our-work)
  - [ğŸ–Šï¸Citing](#ï¸citing)
  - [ğŸ¤Contact Us](#contact-us)
  - [ğŸ’³License](#license)
  - [ğŸ’ªContributing](#contributing)

## âš™ï¸Inference

### Example Code
Here is a code snippet to show you how to play with our model with **HuggingFace** `transformers`:

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = 'DataHammer/hammerllm-1.4b-222k'
text = 'åŒ—äº¬ç†å·¥å¤§å­¦æ˜¯'
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
# if your device donot support the bfloat16, you could remove it
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)

input_ids = tokenizer(text, return_tensors='pt').input_ids
output = model.generate(
    input_ids=input_ids.cuda(),
    max_length=min(int(len(input_ids) + 100), 1024),
    do_sample=True,
    top_p=0.95
).tolist()

generation = tokenizer.decode(output[0])
print(generation)
```

## ğŸ™ŒCases
Here, we show some cases about text completion by our model on Chinese, English, and Code setting.

### Chinese Generation

```
Prompt: åŒ—äº¬ç†å·¥å¤§å­¦æ˜¯
Generation: <s> åŒ—äº¬ç†å·¥å¤§å­¦æ˜¯æ•™è‚²éƒ¨ç›´å±å…¨å›½é‡ç‚¹å¤§å­¦ï¼Œæ˜¯æˆ‘å›½â€œ211å·¥ç¨‹â€å’Œâ€œ985å·¥ç¨‹â€é‡ç‚¹å»ºè®¾çš„æ•™è‚²éƒ¨ç›´å±ç†å·¥ç±»å…¨å›½é‡ç‚¹å¤§å­¦ï¼Œç›´å±äºå·¥ä¸š å’Œä¿¡æ¯åŒ–éƒ¨ï¼Œæ˜¯å…¨å›½é¦–æ‰¹æ­£å¼è·å¾—åšå£«å­¦ä½å’Œåšå£«åç ”ç©¶èµ„æ ¼çš„é«˜æ ¡ä¹‹ä¸€ï¼Œé¦–æ‰¹è·å¾—å…¬å®‰ä¸“ä¸šç¡•å£«ã€å…¬å®‰ä¸“ä¸šåšå£«å’Œå¸æ³•ä¸“ä¸šåšå£«å­¦ä½çš„22æ‰€é«˜æ ¡ä¹‹ ä¸€ï¼Œæ˜¯åšå£«å­¦ä½æˆäºˆå•ä½ã€æ³•å­¦ï¼ˆä¸€çº§å­¦ç§‘ï¼‰å’Œå…¬å®‰å­¦ï¼ˆä¸€çº§å­¦ç§‘ï¼‰åšå£«æˆæƒé«˜æ ¡ï¼Œæ˜¯ä¸€æ‰€ä»¥å·¥ä¸ºä¸»ã€ç†å·¥ç»“åˆã€å¤šå­¦ç§‘åè°ƒå‘å±•çš„å…¨å›½é‡ç‚¹å¤§å­¦ã€‚
åŒ—äº¬ç†å·¥å¤§å­¦æ˜¯ä¸­å¤®éƒ¨é—¨æ‰€å±é«˜æ ¡ï¼Œä¸­å¤®å…±å»ºé«˜æ ¡ï¼Œæ•™è‚²éƒ¨ç›´å±é«˜æ ¡ã€‚
ä¸€ã€å¤§å­¦é™¢ï¼ˆé™¢ç³»ï¼‰
åŒ—äº¬ç†å·¥å¤§å­¦ç°æœ‰48ä¸ªåšå£«å­¦ä½æˆæƒä¸€çº§å­¦ç§‘ï¼Œæ¶µç›–17ä¸ªå­¦ç§‘é—¨ç±»ã€‚
äºŒã€ä¸“ä¸šé™¢ç³»
åŒ—äº¬ç†å·¥å¤§å­¦è®¾æœ‰11ä¸ªä¸€çº§å­¦ç§‘åšå£«åç§‘ç ”æµåŠ¨ç«™
ä¸‰ã€ç ”ç©¶ç”Ÿé™¢
åŒ—äº¬ç†å·¥å¤§å­¦è®¾æœ‰29ä¸ªç ”ç©¶æ‰€ï¼Œå…¶ä¸­å›½å®¶é‡ç‚¹å®éªŒå®¤2ä¸ªï¼› åšå£«åç§‘ç ”æµåŠ¨ç«™18ä¸ªï¼›åšå£«åç§‘ç ”å·¥ä½œç«™3ä¸ªã€‚
åŒ—äº¬ç†å·¥å¤§å­¦ç ”ç©¶ç”Ÿé™¢æ‹›ç”Ÿä¸“ä¸šï¼š</s>
```

```
Prompt: å°ç±³å…¬å¸æ˜¯ä¸€å®¶
Generation: <s> å°ç±³å…¬å¸æ˜¯ä¸€å®¶ä¸“æ³¨äºæ™ºèƒ½ç¡¬ä»¶å’Œç”µå­äº§å“ç ”å‘çš„äº’è”ç½‘å…¬å¸ï¼Œå…¬å¸äº§å“åŒ…æ‹¬æ™ºèƒ½æ‰‹æœºã€ç¬”è®°æœ¬ç”µè„‘ã€è·¯ç”±å™¨ç­‰ã€‚å°ç±³å…¬å¸ä¸€ç›´ä¸“ æ³¨äºæŠ€æœ¯åˆ›æ–°å’Œå“è¶Šçš„ä½“éªŒï¼Œè‡´åŠ›äºä¸ºç”¨æˆ·æä¾›æ›´å¥½çš„äº§å“ã€‚
å°ç±³å…¬å¸æ‹¥æœ‰ä¸€æ”¯ç»éªŒä¸°å¯Œçš„ç ”å‘å›¢é˜Ÿï¼Œåœ¨äº§å“å¼€å‘ã€ç”¨æˆ·ä½“éªŒç­‰æ–¹é¢ç§¯ç´¯äº†ä¸°å¯Œçš„ç»éªŒã€‚å…¬å¸ç ”å‘å›¢é˜Ÿä¸­çš„å¤§éƒ¨åˆ†æˆå‘˜æ¥è‡ªå¾®è½¯ã€è°·æ­Œç­‰çŸ¥åç§‘æŠ€ å…¬å¸ï¼Œå¯¹äº’è”ç½‘å’Œç§»åŠ¨äº’è”ç½‘æŠ€æœ¯æœ‰ç€æ·±åˆ»ç†è§£ã€‚æ­¤å¤–ï¼Œå°ç±³å…¬å¸è¿˜æ‹¥æœ‰ä¸“ä¸šçš„å¼€å‘å›¢é˜Ÿå’Œè‰¯å¥½çš„æŠ€æœ¯æ”¯æŒã€‚
å°ç±³å…¬å¸çš„æ ¸å¿ƒäº§å“ä¹‹ä¸€æ˜¯å°ç±³æ‰‹æœºã€‚å°ç±³å…¬å¸çš„ç›®æ ‡æ˜¯è®©ç”¨æˆ·å¯ä»¥åœ¨æ‰‹æœºä¸­è½»æ¾åˆ†äº«å›¾ç‰‡ã€è§†é¢‘ã€éŸ³ä¹å’ŒéŸ³ä¹æ–‡ä»¶ç­‰ã€‚å°ç±³å…¬å¸æä¾›äº†ä¸€ç§ç®€å•æ˜“ ç”¨çš„ç§»åŠ¨åª’ä½“è§£å†³æ–¹æ¡ˆï¼Œè®©ç”¨æˆ·å¯ä»¥åˆ©ç”¨æ™ºèƒ½æ‰‹æœºæ¥è½»æ¾äº«å—éŸ³ä¹ã€ç…§ç‰‡ã€è§†é¢‘å’Œæ¸¸æˆç­‰ç§»åŠ¨åª’ä½“å†…å®¹ã€‚
å°ç±³æ‰‹æœºè¿˜æ”¯æŒå¤šç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€è¿œç¨‹æ§åˆ¶ã€è¿œç¨‹å®šä½ã€ç§»åŠ¨æ”¯ä»˜ç­‰ã€‚å°ç±³æ‰‹æœºè¿˜æ”¯æŒå„ç§ä¸åŒçš„åº”ç”¨å’ŒæœåŠ¡ï¼ŒåŒ…æ‹¬ç¤¾äº¤åª’ä½“ã€æ¸¸æˆã€ è§†é¢‘åˆ†äº«ã€æœ¬åœ°åŒ–æœåŠ¡ç­‰ã€‚
å°ç±³å…¬å¸è¿˜æ‹¥æœ‰ä¸°å¯Œçš„ç¡¬ä»¶äº§å“çº¿ï¼ŒåŒ…æ‹¬æ™ºèƒ½æ‰‹æœºã€ç¬”è®°æœ¬ç”µè„‘ã€å¹³æ¿ç”µè„‘ã€ç”µè§†ç­‰ã€‚å°ç±³å…¬å¸è¿˜æ¨å‡ºäº†è®¸å¤šæå…·ç«äº‰åŠ›çš„äº§å“ï¼Œå¦‚æ™ºèƒ½æ‰‹ç¯ã€æ™ºèƒ½ æ‰‹è¡¨ç­‰ã€‚</s>
```

### English Generation

```
Prompt: Google is
Generation: <s> Google is to deliver its latest Pixel phone to AT&T, instead of the original, cheaper Pixel 3a.
AT&T US is expected to unveil the Pixel 5a on the 8th of February, or in the evening of Friday the 12th next year.
However, it appears Google has announced the Pixel 5a as the second Pixel phone to be delivered to AT&T, rather than the original Pixel 3a.
According to a blog post published by Pixel US on 26th October, the Pixel 5a will be available on the 9th of March with a price of $599.99.
Google has also promised the Pixel 5a will be the first Pixel phone sold in the U.S., US.
In the announcement made the company is also hinting that the Pixel 5a will be available on Sprint's network, meaning the Pixel 5a won't be delivered to T-Mobile US.
A couple of months ago, the Google Pixel 5a, then known as the Pixel 4a, was revealed to the world on the 22nd of October alongside an invite for beta program for the Pixel 5a.
The Pixel 5a is believed to be the largest smartphone Pixel has ever produced.</s>
```

### Code Generation

```python
Prompt: This is a code snippet for sorting a list of number in Python language with python in-built function `sorted`:
"""python
number_list = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]

Generation: <s> This is a code snippet for sorting a list of number in Python language with python in-built function `sorted`:
"""python
number_list = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
print(sorted(number_list))
"""

[`sorted`](https://docs.python.org/2/library/ordereddict.html) is a function that helps you sort a list of values in Python language.
For the list of items like number, this function is also used to sort the list.
</s>
```

## ğŸ¤—Pretrained Models

We publicly release our checkpoints on Huggingface. Welcome to test and utilize our models!

| Epoch | Step   | Pretrained tokens | Sequence Length | HuggingFace checkpoint |
| ----- | ------ | ----------------- | --------------- | ---------------------- |
| 1     | 25k    | 50B               | 2048            | [DataHammer/hammerllm-1.4b-25k](https://huggingface.co/DataHammer/hammerllm-1.4b-25k)                       |
| 1     | 50k    | 100B              | 2048            | [DataHammer/hammerllm-1.4b-50k](https://huggingface.co/DataHammer/hammerllm-1.4b-50k)                       |
| 1     | 75k    | 150B              | 2048            | [DataHammer/hammerllm-1.4b-75k](https://huggingface.co/DataHammer/hammerllm-1.4b-75k)                       |
| 1     | 100k   | 200B              | 2048            | [DataHammer/hammerllm-1.4b-100k](https://huggingface.co/DataHammer/hammerllm-1.4b-100k)                       |
| 1     | 125k   | 250B              | 2048            | [DataHammer/hammerllm-1.4b-125k](https://huggingface.co/DataHammer/hammerllm-1.4b-125k)                       |
| 1     | 150k   | 300B              | 2048            | [DataHammer/hammerllm-1.4b-150k](https://huggingface.co/DataHammer/hammerllm-1.4b-150k)                       |
| 1     | 172.5k | 345B              | 2048            | [DataHammer/hammerllm-1.4b-172k](https://huggingface.co/DataHammer/hammerllm-1.4b-172k)                       |
| 2     | 197k   | 395B      | 2048            | [DataHammer/hammerllm-1.4b-197k](https://huggingface.co/DataHammer/hammerllm-1.4b-197k)                       |
| 2     | 222k   | 445B     | 2048            | [DataHammer/hammerllm-1.4b-222k](https://huggingface.co/DataHammer/hammerllm-1.4b-222k)                       |

## ğŸ“ŠPerformance

### Perplexity on Latest Test Dataset

For base models that are not instruct-tuned, it is important to show their perplexity on the **latest fresh test dataset**, which reflects their prediction capability without training data leakage. To achieve this goal, we compare the perplexity of our model and some famous sLLMs on famous Chinese and English latest test set. 

Our perplexity calculation follows the settings of **Skywork**, please refer to [this link](https://github.com/SkyworkAI/Skywork?tab=readme-ov-file#é¢†åŸŸæ•°æ®å›°æƒ‘åº¦è¯„ä¼°) for more details about their implementation.

Our model's performance on perplexity (ppl) testing **is second only** to the Qwen-1.5-1.8B model, outperforming Gemma-2B, InterLM2-1.8B, and MiniCPM-2B (350k). Given the fact that the our model size is smaller (1.4B < 1.8B), these experimental results are surprising.

#### Chinese Corpus ([Skywork-ChineseDomainModelingEval](https://huggingface.co/datasets/Skywork/ChineseDomainModelingEval))

|                  | Tokens | Tech      | Movie     | Government | Game      | Finance  | General  | Avg.       |
| ---------------- | ------ | --------- | --------- | ---------- | --------- | -------- | -------- | ---------- |
| Gemma-2B         | 6T     | 71.92     | 271.95    | 93.16      | 122.59    | 46.53    | 74.93    | 92.92      |
| InterLM2-1.8B    | -      | 19.20     | 36.88     | 7.18       | 29.67     | 9.51     | 11.81    | 16.03      |
| MiniCPM-2B(350k) | -      | 17.32     | 36.44     | 7.00       | 26.59     | 7.92     | 10.26    | 14.56      |
| Qwen-1.5(1.8B)   | -      | 16.88     | **33.39** | 6.63       | **25.44** | 7.43     | 9.77     | **13.80**  |
| Qwen-1.5(0.5B)   | -      | 20.70     | 41.59     | 8.24       | 31.64     | 9.41     | 11.98    | 17.13      |
| Ours             | 50B    | 19.59     | 44.58     | 7.48       | 33.55     | 8.10     | 11.31    | 16.49      |
| Ours             | 100B   | 18.23     | 40.26     | 7.11       | 30.80     | 7.42     | 10.47    | 15.23      |
| Ours             | 150B   | 17.59     | 38.86     | 6.91       | 29.76     | 7.19     | 10.20    | 14.75      |
| Ours             | 200B   | 17.24     | 38.03     | 6.84       | 29.01     | 7.05     | 10.04    | 14.48      |
| Ours             | 250B   | 17.08     | 37.53     | 6.81       | 28.63     | 6.97     | 9.91     | 14.32      |
| Ours             | 300B   | 16.92     | 37.17     | 6.82       | 28.37     | 6.89     | 9.82     | 14.21      |
| Ours             | 345B   | 16.79     | 36.89     | 6.73       | 28.11     | 6.84     | 9.75     | 14.09      |
| Ours(epoch-2)    | 395B   | 16.71     | 36.64     | **6.60**   | 28.01     | 6.85     | 9.72     | 14.00      |
| Ours(epoch-2)    | 445B   | **16.67** | 36.43     | 6.68       | 27.86     | **6.75** | **9.65** | 13.95      |


#### English Corpus ([RealTimeData-Latest Collection](https://huggingface.co/collections/RealTimeData/latest-collection-65e1c5d70e180e6263f82589))

|                  | Tokens   | arxiv-latest | bbc-latest | github-latest | wikitext-latest | Avg.       |
| ---------------- | -------- | ------------ | ---------- | ------------- | --------------- | ---------- |
| TinyLlama-1.1B   | 3T       | 8.21         | **8.27**   | **5.90**      | **7.38**        | **7.37**   |
| Gemma-2B         | 6T       | 28.96        | 37.83      | 22.10         | 28.11           | 28.72      |
| Phi-1.5(1.3B)    | 30B \* 5 | 13.08        | 14.07      | 10.42         | 15.16           | 13.06      |
| InterLM2-1.8B    | -        | **7.84**     | 8.75       | 6.87          | 8.52            | 7.96       |
| MiniCPM-2B(350k) | -        | 10.81        | 9.15       | **nan**       | 8.08            | 9.28       |
| Qwen-1.5(1.8B)   | -        | 8.70         | 8.92       | **nan**       | 8.75            | 8.79       |
| Qwen-1.5(0.5B)   | -        | 10.42        | 10.93      | **nan**       | 10.58           | 10.64      |
| Ours             | 50B      | 10.67        | 13.19      | 9.14          | 10.47           | 10.77      |
| Ours             | 100B     | 9.87         | 11.97      | 8.46          | 9.71            | 9.93       |
| Ours             | 150B     | 9.57         | 11.63      | 8.16          | 9.38            | 9.61       |
| Ours             | 200B     | 9.41         | 11.43      | 8.06          | 9.21            | 9.45       |
| Ours             | 250B     | 9.39         | 11.21      | 7.96          | 9.13            | 9.35       |
| Ours             | 300B     | 9.79         | 11.24      | 7.90          | 9.03            | 9.41       |
| Ours             | 345B     | 9.59         | 11.11      | 7.86          | 9.04            | 9.33       |
| Ours(epoch-2)    | 395B     | 9.65         | 11.06      | 7.86          | 9.01            | 9.32       |
| Ours(epoch-2)    | 445B     | 9.34         | 11.07      | 7.86          | 9.04            | 9.26       |

**NOTE**: Average PPL of Qwen-1.5 and MiniCPM models have excluded GitHub dataset for NaN value.

### ğŸ““Tokenizer

To reveal the effectiveness of our tokenizer, we compare it with some famous open-source LLMs' tokenizers by using following metrics:
1. **Compression Rate:** We compare two kinds of compression rate of tokenizer with tokenizers of some open-source LLMs:
   * [Byte per token compression rate](https://kexue.fm/archives/9752#%E6%95%88%E6%9E%9C%E6%B5%8B%E8%AF%95) 
   * [Compared compression](https://arxiv.org/pdf/2309.16609.pdf) that measures the advantage over base Llama-2-7B tokenizer. 

2. **Chinese Character Coverage:** a good Chinese LLM should cover more characters. In our work, we leverage [vocab-coverage](https://github.com/twang2218/vocab-coverage) for computing the coverage of Chinese characters, which includes:
     * **F**irst-level **C**hinese character **C**overage (FCC) contains 3500 widely-used Chinese characters.
     * **S**econd-level **C**hinese character **C**overage (SCC) contains 3000 Chinese characters.
     * **T**hird-level **C**hinese character **C**overage (TCC) contains 1605 uncommon Chinese characters.
     
The experimental results are shown as follows:

|     Tokenizer      | FCC | SCC | TCC | Byte per Token $\uparrow$ | Comparied Compression $\downarrow$ |
|:------------------:|:-----------:|:-----------:|:-----------:|:-----------:|:----------------:|
| [Chatglm-6b](https://huggingface.co/THUDM/chatglm-6b)     |   99.97%    |   57.47%    |    2.99%    | 4.2911      |     0.5303     | 
| [Chatglm2/3-6b](https://huggingface.co/THUDM/chatglm2-6b)    |   **100.00%**  |   77.83%    |   13.89%    | 4.0329      |     0.5642     |  
| [Baichuan2-7b/14b](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat)  |   **100.00%**   |    99.8%    |   86.48%    | 4.1827      |     0.5440     | 
| [Internlm-7b/20b](https://huggingface.co/internlm/internlm-7b)   |   **100.00%**   |   65.93%    |    5.67%    | 4.3133      |     0.5276     |   
| [Qwen-7b/14b/72b](https://huggingface.co/Qwen/Qwen-7B)   |   **100.00%**   |   **100.00%**   |   **100.00%**   | 4.1326      |     0.5506     |   
| [Llama-2-7b/13b/70b](https://huggingface.co/meta-llama/Llama-2-7b-hf)  |   17.29%    |    0.13%    |    0.00%    | 2.2755      |        1.00         |  
| [Ours](https://github.com/Academic-Hammer/HammerLLM/tree/main/merge_tokenizer/internlm_merged_fast)   |    **100.00%**     |   **100.00%**    |    **100.00%**    |   **4.3143**   |     **0.5274**             | 

The experimental results reveal the advantages of our tokenizer over existing popular LLMs' tokenizers in compression rate (on Chinese, English, and Code), and the Chinese character coverage. Please refer to [REPRODUCE.md](./REPRODUCE.md) for more details about this experiment.

### â©Acceleration

With the help of these two stragegies, we could achieve a high throughput of **16k tokens per GPU per second** for training:
- [Flash Attention 2](https://github.com/Dao-AILab/flash-attention)
- [torch.compile](https://pytorch.org/docs/stable/torch.compiler.html)

The setting for computing the throughput is:
* ZeRO-1
* block_size: 2048
* per_device_train_batch_size: 8
* gradient_accumulation_steps: 16

| Settings                          | tokens per GPU per second |
|-----------------------------------|---------------------------|
| None                              | CUDA OOM                  |
| Flash Attention 2                 | 13k                       |
| torch.compile                     | CUDA OOM                  |
| Flash Attention 2 + torch.compile | 16k                       |

To cover more Chinese characters, our tokenizer is much larger than [TinyLlama](https://github.com/jzhang38/TinyLlama) (105789 > 32000), leading to the lower throughput than TinyLlama (16k < 24k). 

However, our throughput is comparable with TinyLlama, when the size of the tokenizer is the same (per_device_train_batch_size is set to 20 for this).

| Settings                          | tokens per GPU per second |
|-----------------------------------|---------------------------|
| Flash Attention 2 + torch.compile | 24k                       |

Unlike TinyLlama that leverages some complex the operations fusion, we achieve this throughput solely based on `torch.compile` and `flash attention 2`.


## ğŸ“ˆTraining Progress

So far, we have optimized our model on over 400B Chinese, English, and Code tokens.
Perplexity results in Chinese and English test set demonstrate that our model has not yet converged, hence we are currently continuing to optimize it. 

We will periodically upload and make our checkpoints publicly available for analysis and research by the open-source community.

The trend of **loss** changes during the training process is shown in the following figureï¼š
![Training Loss](.github/images/train_loss.jpg)

The trends of standardized **perplexity** variation based on Skywork series models on the **Skywork-ChineseDomainModelingEval** and **RealTimeData-Latest Collection** datasets are:

<table rules="none" align="center">
	<tr>
		<td>
			<center>
				<img src=".github/images/zh-ppl.jpg" />
				<br/>
				<center>Chinese PPL</center>
      </center>
		</td>
		<td>
			<center>
				<img src=".github/images/en-ppl.jpg" />
        <br/>
				<center>English PPL</center>
			</center>
		</td>
	</tr>
</table>



## â™»ï¸Reproduce Our Work

One of contriution is that we have publicly release the steps to train our model from scratch. You can refer to [this file](./REPRODUCE.md) for more details about our work.



## ğŸ–Šï¸Citing

```
@software{Ziao_HammerLLM_2024,
author = {Ziao, Ma and Tian, Lan and Yang, Yizhe and Yong, Hu},
month = apr,
title = {{HammerLLM}},
url = {https://github.com/Academic-Hammer/HammerLLM},
version = {1.0.0},
year = {2024}
}
```

## ğŸ¤Contact Us

You can contact us through the email address: 
* maziaoylwt@gmail.com
* lantiangmftby@gmail.com


## ğŸ’³License


This project is released under the [MIT license](./LICENSE).

## ğŸ’ªContributing

<!-- Guidelines for how others can contribute to this repository, including coding standards, pull requests, and other relevant information. -->

We hope this documentation is helpful for understanding and utilizing our LLM model effectively. For further questions or support, please open an issue on this repository.
